{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8923367,"sourceType":"datasetVersion","datasetId":5367244}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# USAGE: python process_asr_text_tokenizer.py --manifest=<path to train manifest files, seperated by commas> \\\n#         --data_root=\"<output directory>\" \\\n#         --vocab_size=<number of tokens in vocabulary> \\\n#         --tokenizer=<\"spe\" or \"wpe\"> \\\n#         --log\n# where <manifest> can be: train_clean_100, train_clean_360, train_other_500\n# You can also put more than one data_set comma-separated:\n# --manifest=\"train_clean_100,train_clean_360,train_other_500\"\n# or\n#       python process_asr_text_tokenizer.py --data_file=<path to train text file> \\\n#         --data_root=\"<output directory>\" \\\n#         --vocab_size=<number of tokens in vocabulary> \\\n#         --tokenizer=<\"bpe\" or \"wpe\"> \\\n#         --log\n# where <manifest> can be: train_clean_100, train_clean_360, train_other_500\n# You can also put more than one data_set comma-separated:\n# --manifest=\"train_clean_100,train_clean_360,train_other_500\"\n#\n# Args:\n#   --manifest or --data_file: If your text data lies inside of an ASR manifest file,\n#       then use the --manifest path. If instead the text data is inside a file with separate lines\n#       corresponding to different text lines, then use --data_file.\n#       In either case, you can add commas to concatenate different manifests or different data files.\n#\n#   --data_root: The output directory (whose subdirectories will be created if not present) where\n#       the tokenizers will be placed.\n#\n#   --vocab_size: The size of the tokenizer vocabulary. Larger vocabularies can accommodate almost entire,\n#       words but the decoder size of any model will grow proportionally.\n#\n#   --tokenizer: Can be either spe or wpe . spe refers to the Google sentencepiece library tokenizer.\n#       wpe refers to the HuggingFace BERT Word Piece tokenizer.\n#\n#   --no_lower_case: When this flag is passed, it will force the tokenizer to create seperate tokens for\n#       upper and lower case characters. By default, the script will turn all the text to lower case\n#       before tokenization (and if upper case characters are passed during training/inference, the\n#       tokenizer will emit a token equivalent to Out-Of-Vocabulary). Used primarily for the\n#       English language.\n#\n#    --spe_type: The sentencepiece library has a few implementations of the tokenization technique, and\n#       spe_type refers to these implementations. Currently supported types are unigram, bpe, char, word.\n#       Defaults to bpe.\n#\n#   --spe_character_coverage: The sentencepiece library considers how much of the original vocabulary it\n#       should cover in its \"base set\" of tokens (akin to the lower and upper case characters of the\n#       English language). For almost all languages with small base token sets (<1000 tokens), this\n#       should be kept at its default of 1.0. For languages with larger vocabularies (say Japanese,\n#       Mandarin, Korean etc), the suggested value is 0.9995.\n#\n#   --spe_user_defined_symbols: The sentencepiece library allows you to add your own tokens to the base set.\n#      This flag allows you to pass a space separated list of tokens that you want to add to the base set.\n#      These tokens remain in the decoded text and are encoded automatically when present in the input text.\n#\n#   --spe_control_symbols: The sentencepiece library allows you to add your own tokens to the base set.\n#      This flag allows you to pass a space separated list of tokens that you want to add to the base set.\n#      These tokens get removed at decode time and are not encoded from the text - can only be added to the\n#      input programatically.\n#\n#   --spe_byte_fallback: If <unk>, fallback to a byte sequence of the characters.\n#\n#   --spe_split_digits: If true, digits are split into individual tokens.\n#\n#   --spe_sample_size: If the dataset is too large, consider using a sampled dataset indicated by a\n#       positive integer. By default, any negative value (default = -1) will use the entire dataset.\n#\n#   --spe_train_extremely_large_corpus: When training a sentencepiece tokenizer on very large amounts of text,\n#       sometimes the tokenizer will run out of memory or wont be able to process so much data on RAM.\n#       At some point you might receive the following error - \"Input corpus too large, try with\n#       train_extremely_large_corpus=true\". If your machine has large amounts of RAM, it might still be possible\n#       to build the tokenizer using the above flag. Will silently fail if it runs out of RAM.\n#\n#   --spe_max_sentencepiece_length: Limits the maximum length that any any SentencePiece subword can be.\n#       Using this will change the subword tokens generated.\n#\n#   --spe_pad: Adds <pad> as special token.\n#\n#   --spe_bos: Adds <s> as Begining-of-Sentence special token.\n#\n#   --spe_eos: Adds </s> as End-of-Sentence special token.\n#\n#   --log: Whether the script should display log messages\n\n\nimport argparse\nimport json\nimport logging\nimport os\nfrom typing import List, Optional\n\nimport tokenizers\n\nfrom nemo.collections.common.tokenizers.sentencepiece_tokenizer import create_spt_model\nfrom nemo.utils.data_utils import DataStoreObject\n\nparser = argparse.ArgumentParser(description='Create tokenizer')\ngroup = parser.add_mutually_exclusive_group(required=True)\ngroup.add_argument(\"--manifest\", default=None, type=str, help='Comma separated list of manifest files')\ngroup.add_argument(\"--data_file\", default=None, help='data file from which to create tokenizer model')\nparser.add_argument(\"--data_root\", required=True, default=None, type=str, help='Output directory')\nparser.add_argument(\"--vocab_size\", default=1024, type=int, help='Vocabulary size')\nparser.add_argument(\"--tokenizer\", default=\"wpe\", choices=[\"spe\", \"wpe\"], help='Type of tokenization to perform')\nparser.add_argument(\n    \"--spe_type\",\n    default=\"bpe\",\n    choices=['bpe', 'unigram', 'char', 'word'],\n    help='Type of the SentencePiece model. Can be `bpe`, `unigram`, `char` or `word`.'\n    'Used only if --tokenizer == `spe`',\n)\nparser.add_argument(\n    '--spe_character_coverage',\n    type=float,\n    default=1.0,\n    help=\"Character coverage percentage for SentencePiece tokenization. For languages \"\n    \"with large vocabulary, should be close to 0.9995, otherwise kept as 1.0\",\n)\nparser.add_argument('--spe_bos', action='store_true', help='Add <s> token to SentencePiece Tokenizer.')\nparser.add_argument('--spe_eos', action='store_true', help='Add </s> token to SentencePiece Tokenizer.')\nparser.add_argument('--spe_pad', action='store_true', help='Add <pad> token to SentencePiece Tokenizer.')\nparser.add_argument(\n    '--spe_user_defined_symbols', default=None, type=str, nargs='+', help='User defined symbols for SentencePiece'\n)\nparser.add_argument(\n    '--spe_control_symbols', default=None, type=str, nargs='+', help='Control symbols for SentencePiece'\n)\nparser.add_argument('--spe_split_digits', action='store_true', help='Split digits into separate tokens.')\n\nparser.add_argument(\n    '--spe_sample_size',\n    type=int,\n    default=-1,\n    help=\"Samples the dataset by `sample_size` if positive integer, otherwise uses whole dataset\",\n)\nparser.add_argument('--spe_train_extremely_large_corpus', action='store_true', help='')\nparser.add_argument(\n    '--spe_max_sentencepiece_length',\n    type=int,\n    default=-1,\n    help='Limit the maximum number of tokens in each SentencePiece subword. '\n    'Must be a positive integer > 0. By default places no limit on subword length.',\n)\nparser.add_argument(\n    '--spe_no_split_by_unicode_script',\n    dest='spe_split_by_unicode_script',\n    action='store_false',\n    help=\"Don't use Unicode script to split sentence pieces.\",\n)\nparser.add_argument(\n    '--spe_byte_fallback',\n    dest='spe_byte_fallback',\n    action='store_true',\n    help=\"If <unk>, fallback to a byte sequence of the characters.\",\n)\nparser.add_argument('--no_lower_case', dest='lower_case', action='store_false')\nparser.add_argument(\"--log\", action='store_true')\nparser.set_defaults(log=False, lower_case=True, spe_train_extremely_large_corpus=False)\nargs = parser.parse_args()\n\n\ndef __build_document_from_manifests(\n    data_root: str, manifests: str,\n):\n    if ',' in manifests:\n        manifests = manifests.split(',')\n    else:\n        manifests = [manifests]\n\n    document_dir = os.path.join(data_root, 'text_corpus')\n    if not os.path.exists(document_dir):\n        os.makedirs(document_dir)\n\n    document_path = os.path.join(document_dir, 'document.txt')\n\n    if os.path.exists(document_path):\n        logging.info('Corpus already exists at path : %s', document_path)\n        return document_path\n\n    num_lines = 0\n    with open(document_path, 'w') as out_writer:\n        for manifest in manifests:\n            with open(DataStoreObject(manifest).get(), 'r') as in_reader:\n                for line in in_reader:\n                    item = json.loads(line)\n                    text = item['text']\n\n                    out_writer.write(text + '\\n')\n                    out_writer.flush()\n\n                    num_lines += 1\n\n            logging.info(f\"Finished extracting manifest : {manifest}\")\n\n        logging.info(\"Finished extracting all manifests ! Number of sentences : {}\".format(num_lines))\n    return document_path\n\n\ndef __process_data(\n    text_path: str,\n    dst_folder: str,\n    vocab_size: int,\n    tokenizer_type: str,\n    spe_type: str,\n    spe_character_coverage: float,\n    spe_train_extremely_large_corpus: bool,\n    spe_sample_size: int,\n    spe_max_sentencepiece_length: int,\n    spe_split_by_unicode_script: bool,\n    spe_bos: bool,\n    spe_eos: bool,\n    spe_pad: bool,\n    spe_control_symbols: Optional[List[str]],\n    spe_user_defined_symbols: Optional[List[str]],\n    spe_byte_fallback: bool,\n    spe_split_digits: bool,\n    lower_case: bool,\n):\n    \"\"\"\n    Converts flac to wav and build manifests's json\n    Args:\n        text_path: source with text lines\n        dst_folder: where wav files will be stored\n        vocab_size: vocabular size used in encoding the text\n        tokenizer_type: type of tokenization to perform - wpe or spe\n        spe_type: type of tokenization model used for spe.\n        spe_character_coverage: float value between 0 and 1 (as a percentage). For languages with a vast charset,\n            can be < 1.0, but for all other languages, it should be set as 1.0\n        spe_sample_size: int, default of -1. If positive integer is used, samples the dataset\n            by given sample size.\n        spe_train_extremely_large_corpus: bool. If dataset is too large, and user has sufficient RAM,\n            this flag can be set to try to trained the tokenizer. Will silently fail if it runs out of RAM.\n        spe_max_sentencepiece_length: Limits the maximum length of the SentencePiece subword that can be constructed.\n            By default, no limit is placed.\n        spe_bos: Bool flag, whether to add <s> to SentencePiece tokenizer vocabulary.\n        spe_eos: Bool flag, whether to add </s> to SentencePiece tokenizer vocabulary.\n        spe_pad: Bool flag, whether to add <pad> to SentencePiece tokenizer vocabulary.\n        spe_control_symbols: control symbols to add to tokenizer, as defined by sentencepiece.\n            These tokens get removed at decode time and are not encoded from the text - can only be added to the input programatically.\n        spe_user_defined_symbols: user symbols to add to tokenizer, as defined by sentencepiece.\n            These tokens remain in the decoded text and are encoded automatically when present in the input text.\n        spe_byte_fallback: If <unk>, fallback to a byte sequence of the character.\n        spe_split_digits: If true, digits are split into individual tokens.\n        lower_case: whether to tokenize with lower case character set only (for english)\n\n    Returns:\n    \"\"\"\n    if tokenizer_type == 'spe':\n\n        # Prepare directory of tokenizer\n        if spe_max_sentencepiece_length > 0:\n            tokenizer_dir = os.path.join(dst_folder, 'tokenizer_{}_{}_v{}_max_{}').format(\n                tokenizer_type, spe_type, vocab_size, spe_max_sentencepiece_length\n            )\n        else:\n            tokenizer_dir = os.path.join(dst_folder, 'tokenizer_{}_{}_v{}').format(\n                tokenizer_type, spe_type, vocab_size\n            )\n\n        if spe_pad:\n            tokenizer_dir = f'{tokenizer_dir}_pad'\n        if spe_bos:\n            tokenizer_dir = f'{tokenizer_dir}_bos'\n        if spe_eos:\n            tokenizer_dir = f'{tokenizer_dir}_eos'\n\n        if not os.path.exists(tokenizer_dir):\n            os.makedirs(tokenizer_dir)\n\n        if os.path.exists(os.path.join(tokenizer_dir, 'tokenizer.model')):\n            logging.warning(\"Model file already exists, overriding old model file !\")\n            os.remove(os.path.join(tokenizer_dir, 'tokenizer.model'))\n\n        # Build tokenizer\n        tokenizer_path, vocab_path = create_spt_model(\n            data_file=text_path,\n            vocab_size=vocab_size,\n            sample_size=spe_sample_size,\n            do_lower_case=lower_case,\n            output_dir=tokenizer_dir,\n            tokenizer_type=spe_type,\n            character_coverage=spe_character_coverage,\n            train_extremely_large_corpus=spe_train_extremely_large_corpus,\n            max_sentencepiece_length=spe_max_sentencepiece_length,\n            split_by_unicode_script=spe_split_by_unicode_script,\n            bos=spe_bos,\n            eos=spe_eos,\n            pad=spe_pad,\n            control_symbols=spe_control_symbols,\n            user_defined_symbols=spe_user_defined_symbols,\n            byte_fallback=spe_byte_fallback,\n            split_digits=spe_split_digits,\n        )\n\n    else:\n        tokenizer_dir = os.path.join(dst_folder, 'tokenizer_{}_v{}').format(tokenizer_type, vocab_size)\n\n        if not os.path.exists(tokenizer_dir):\n            os.makedirs(tokenizer_dir)\n\n        tokenizer = tokenizers.BertWordPieceTokenizer(lowercase=lower_case)\n\n        tokenizer.train(text_path, vocab_size=vocab_size)\n        tokenizer.save_model(tokenizer_dir)\n\n    return tokenizer_dir\n\n\ndef main():\n    data_root = args.data_root\n    manifests = args.manifest\n    data_file = args.data_file\n    vocab_size = args.vocab_size\n    tokenizer = args.tokenizer\n    spe_type = args.spe_type\n    spe_character_coverage = args.spe_character_coverage\n    spe_sample_size = args.spe_sample_size\n    spe_train_extremely_large_corpus = args.spe_train_extremely_large_corpus\n    spe_max_sentencepiece_length = args.spe_max_sentencepiece_length\n    spe_split_by_unicode_script = args.spe_split_by_unicode_script\n    spe_bos, spe_eos, spe_pad = args.spe_bos, args.spe_eos, args.spe_pad\n    spe_control_symbols = args.spe_control_symbols\n    spe_user_defined_symbols = args.spe_user_defined_symbols\n    spe_byte_fallback = args.spe_byte_fallback\n    spe_split_digits = args.spe_split_digits\n    lower_case = args.lower_case\n\n    if not os.path.exists(data_root):\n        os.makedirs(data_root)\n\n    if args.log:\n        logging.basicConfig(level=logging.INFO)\n\n    if manifests:\n        text_corpus_path = __build_document_from_manifests(data_root, manifests)\n    else:\n        text_corpus_path = data_file\n    tokenizer_path = __process_data(\n        text_corpus_path,\n        data_root,\n        vocab_size,\n        tokenizer,\n        spe_type,\n        lower_case=lower_case,\n        spe_character_coverage=spe_character_coverage,\n        spe_sample_size=spe_sample_size,\n        spe_train_extremely_large_corpus=spe_train_extremely_large_corpus,\n        spe_max_sentencepiece_length=spe_max_sentencepiece_length,\n        spe_split_by_unicode_script=spe_split_by_unicode_script,\n        spe_bos=spe_bos,\n        spe_eos=spe_eos,\n        spe_pad=spe_pad,\n        spe_control_symbols=spe_control_symbols,\n        spe_user_defined_symbols=spe_user_defined_symbols,\n        spe_byte_fallback=spe_byte_fallback,\n        spe_split_digits=spe_split_digits,\n    )\n\n    print(\"Serialized tokenizer at location :\", tokenizer_path)\n    logging.info('Done!')\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"1ec1a1f8-f57c-44f7-bdb3-3eb5f28af6d0","_cell_guid":"0999fac0-de98-44d8-aa84-1a7462960dce","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}